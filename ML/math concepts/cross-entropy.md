# What is Cross-Entropy?

Cross-entropy gives us a way to express how different two probability distributions are. Values of the top multiplited by the a logarithms of the values of the bottom. element by element, then you sum all of the numbers across the whole vector. It has a minus sign  because the value are between 0 and 1. 
